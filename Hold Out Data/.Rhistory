cv.lda <-
function (data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=123) {
n <- nrow(data)
set.seed(seed)
datay=data[,yname] #response variable
library(MASS)
#partition the data into K subsets
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n)
#generate indices 1:10 and sample n of them
# K fold cross-validated error
CV=NULL
auc_means <- rep(0,10)
detection_rate <- rep(0,10)
false_alarm_rate <- rep(0,10)
for (i in 1:K) { #i=1
test.index <- seq_len(n)[(s == i)] #test data
train.index <- seq_len(n)[(s != i)] #training data
#model with training data
lda.fit=lda(model, data=data[train.index,])
#observed test set y
lda.y <- data[test.index, yname]
#predicted test set y
lda.predy=predict(lda.fit, data[test.index,])$class
#observed - predicted on test data
error= mean(lda.y!=lda.predy)
#error rates
CV=c(CV,error)
#auc curve
library(ROCR)
pred.k = prediction(predict(lda.fit, data[test.index,])$posterior[,2], data[test.index,]$is_blue_tarp)
auc.k = performance(pred.k, "auc")
auc_means[i] <- auc.k@y.values[[1]]
a <-ifelse(data[test.index,]$is_blue_tarp == 'False', 1,0)
p <- ifelse(lda.predy == "False", 1,0)
df = data.frame(a,p)
h <- custom_err(df)
detection_rate[i] <- h$d_rate
false_alarm_rate[i] <- h$f_rate
}
#Output
list(call = model, K = K,
lda_error_rate = mean(CV),
seed = seed, auc_mean = mean(auc_means),
d_rate = mean(detection_rate), f_rate = mean(false_alarm_rate))
}
er2<-cv.lda(data = haiti_data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=17)
1-er2$lda_error_rate
er2$auc_mean
er2$d_rate
er2$f_rate
#QDA
cv.qda <-
function (data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=123) {
n <- nrow(data)
set.seed(seed)
datay=data[,yname] #response variable
library(MASS)
#partition the data into K subsets
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n)
#generate indices 1:10 and sample n of them
# K fold cross-validated error
CV=NULL
auc_means <- rep(0,10)
detection_rate <- rep(0,10)
false_alarm_rate <- rep(0,10)
for (i in 1:K) { #i=1
test.index <- seq_len(n)[(s == i)] #test data
train.index <- seq_len(n)[(s != i)] #training data
#model with training data
qda.fit=qda(model, data=data[train.index,])
#observed test set y
qda.y <- data[test.index, yname]
#predicted test set y
qda.predy=predict(qda.fit, data[test.index,])$class
#observed - predicted on test data
error= mean(qda.y!=qda.predy)
#error rates
CV=c(CV,error)
#auc curve
library(ROCR)
pred.k = prediction(predict(qda.fit, data[test.index,])$posterior[,2], data[test.index,]$is_blue_tarp)
auc.k = performance(pred.k, "auc")
auc_means[i] <- auc.k@y.values[[1]]
a <-ifelse(data[test.index,]$is_blue_tarp == 'False', 1,0)
p <- ifelse(qda.predy == "False", 1,0)
df = data.frame(a,p)
h <- custom_err(df)
detection_rate[i] <- h$d_rate
false_alarm_rate[i] <- h$f_rate
}
#Output
list(call = model, K = K,
qda_error_rate = mean(CV), seed = seed,
seed = seed, auc_mean = mean(auc_means),
d_rate = mean(detection_rate), f_rate = mean(false_alarm_rate))
}
er1<-cv.qda(data = haiti_data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=17)
1-er1$qda_error_rate
er1$auc_mean
er1$d_rate
er1$f_rate
#K NNN
cv.knn <-
function (data, k_fold=10, k_value=1, seed=123) {
n <- nrow(data)
set.seed(seed)
library(class)
#partition the data into K subsets
f <- ceiling(n/k_fold)
s <- sample(rep(1:k_fold, f), n)
#generate indices 1:10 and sample n of them
# K fold cross-validated error
CV=NULL
detection_rate <- rep(0,10)
false_alarm_rate <- rep(0,10)
for (i in 1:k_fold) { #i=1
test.index <- seq_len(n)[(s == i)] #test data
train.index <- seq_len(n)[(s != i)] #training data
train.X <- data.frame(data$Red[train.index], data$Green[train.index], data$Blue[train.index])
test.X <- data.frame(data$Red[test.index], data$Green[test.index], data$Blue[test.index])
train.is_blue_tarp <- data$is_blue_tarp[train.index]
test.is_blue_tarp <- data$is_blue_tarp[test.index]
knn.pred = knn(train.X, test.X, train.is_blue_tarp, k=k_value)
#observed - predicted on test data
error= mean(knn.pred!=test.is_blue_tarp)
#error rates
CV=c(CV,error)
a <-ifelse(test.is_blue_tarp == 'False', 1,0)
p <- ifelse(knn.pred == "False", 1,0)
df = data.frame(a,p)
h <- custom_err(df)
detection_rate[i] <- h$d_rate
false_alarm_rate[i] <- h$f_rate
}
#Output
list(K = k_value,
knn_error_rate = mean(CV), seed = seed,
d_rate = mean(detection_rate), f_rate = mean(false_alarm_rate))
}
er1<-cv.knn(data = haiti_data, k_fold=10, k_value=6, seed=17)
1-er1$knn_error_rate
er1$d_rate
er1$f_rate
library(ROCR)
#SVM
library("pROC")
cv.svm <-
function (data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=123, kernel = 'linear') {
n <- nrow(data)
set.seed(seed)
datay=data[,yname] #response variable
library(e1071)
#partition the data into K subsets
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n)
#generate indices 1:10 and sample n of them
# K fold cross-validated error
CV=NULL
auc_means <- rep(0,10)
detection_rate <- rep(0,10)
false_alarm_rate <- rep(0,10)
for (i in 1:K) { #i=1
test.index <- seq_len(n)[(s == i)] #test data
train.index <- seq_len(n)[(s != i)] #training data
#model with training data
svm.fit <- svm(model, kernel = 'radial', data = data[train.index,], cost = 5, gamma = 50) #update cost
#observed test set y
svm.y <- data[test.index, yname]
#predicted test set y
svm.predy = predict(svm.fit, data[test.index,])
#observed - predicted on test data
error= mean(svm.y!=svm.predy)
#error rates
CV=c(CV,error)
#auc curve
#HAVEN"T FIGURED THIS PART OUT
#pred.k = prediction(predict(lda.fit, data[test.index,])$posterior[,2], data[test.index,]$is_blue_tarp)
#auc.k = performance(pred.k, "auc")
#auc_means[i] <- auc.k@y.values[[1]]
a <-ifelse(data[test.index,]$is_blue_tarp == 'False', 1,0)
p <- ifelse(svm.predy == "False", 1,0)
df = data.frame(a,p)
h <- custom_err(df)
auc_means[i]<-auc(svm.predy, df$a)
detection_rate[i] <- h$d_rate
false_alarm_rate[i] <- h$f_rate
}
#Output
list(call = model, K = K,
svm_error_rate = mean(CV),
seed = seed, auc_mean = mean(auc_means),
d_rate = mean(detection_rate), f_rate = mean(false_alarm_rate))
}
er2<-cv.svm(data = haiti_data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=17, kernel = "radial")
er2$svm_error_rate
er2$auc_mean
er2$d_rate
er2$f_rate
indexes = createDataPartition(hait_data$is_blue_tarp, p = .66, list = F)
indexes = createDataPartition(haiti_data$is_blue_tarp, p = .66, list = F)
train = haiti_data[indexes,]
test = haiti_data[-indexes,]
rf.fit <-randomForest(is_blue_tarp ~ Red + Green + Blue,data=train,mtry=1, ntree = 250)
rf.predy = predict(rf.fit,data=test)
length(rf.predy)
rf.predy = predict(rf.fit,data=test, type = 'is_blue_tarp')
rf.predy = predict(rf.fit,data=test, type = 'response')
length(rf.predy)
as.factor(rf.predy)
confusionMatrix(train$is_blue_tarp, as.factor(rf.predy))
confusionMatrix(test$is_blue_tarp, as.factor(rf.predy))
rf.predy = predict(rf.fit, data=test, type = 'response')
as.factor(rf.predy)
confusionMatrix(test$is_blue_tarp, as.factor(rf.predy))
test = test['Red', 'Green', 'Blue', 'is_blue_tarp']
test = test['Red', 'Green', 'Blue']
test = haiti_data[-indexes,]
View(rf.fit)
library(ISLR)
train = 1:200
Hitters.train = Hitters[train, ]
Hitters.test = Hitters[-train, ]
library(randomForest)
rf.hitters = randomForest(Salary ~ ., data = Hitters.train, ntree = 500, mtry = 19)
rf.pred = predict(rf.hitters, Hitters.test)
Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
Hitters.train = Hitters[train, ]
Hitters.test = Hitters[-train, ]
rf.hitters = randomForest(Salary ~ ., data = Hitters.train, ntree = 500, mtry = 19)
rf.pred = predict(rf.hitters, Hitters.test)
setwd("~/uva_dsi/SYS6018/project1")
haiti_data = read.csv("HaitiPixels.csv")
length(unique(haiti_data$Class))
#Vegetation, Soil, Rooftop, Varioous Non-Tarp, Blue Tarp
#I'm first going to create a factor that is true if blue tarp and false otherwise
haiti_data$is_blue_tarp <- factor(ifelse(haiti_data$Class == "Blue Tarp", "True", "False"))
indexes = createDataPartition(haiti_data$is_blue_tarp, p = .66, list = F)
train = haiti_data[indexes,]
test = haiti_data[-indexes,]
rf.fit <-randomForest(is_blue_tarp ~ Red + Green + Blue, data=train, mtry=1, ntree = 250)
rf.predy = predict(rf.fit, test, type = 'response')
as.factor(rf.predy)
confusionMatrix(test$is_blue_tarp, as.factor(rf.predy))
cv.rf <-
function (data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=123) {
n <- nrow(data)
set.seed(seed)
datay=data[,yname] #response variable
#partition the data into K subsets
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n)
#generate indices 1:10 and sample n of them
# K fold cross-validated error
CV=NULL
auc_means <- rep(0,10)
detection_rate <- rep(0,10)
false_alarm_rate <- rep(0,10)
for (i in 1:K) { #i=1
test.index <- seq_len(n)[(s == i)] #test data
train.index <- seq_len(n)[(s != i)] #training data
#model with training data
rf.fit <-randomForest(model,data=data[train.index,],mtry=1, ntree = 250)
#predict
nrow(data[test.index,])
rf.predy = predict(rf.fit,data[test.index,])
error= mean(data[test.index, yname]!=as.factor(rf.predy))
#error rates
CV=c(CV,error)
#auc curve
#HAVEN"T FIGURED THIS PART OUT
#pred.k = prediction(predict(lda.fit, data[test.index,])$posterior[,2], data[test.index,]$is_blue_tarp)
#auc.k = performance(pred.k, "auc")
#auc_means[i] <- auc.k@y.values[[1]]
a <-ifelse(data[test.index,]$is_blue_tarp == 'False', 1,0)
p <- ifelse(as.factor(rf.predy) == "False", 1,0)
df = data.frame(a,p)
h <- custom_err(df)
#auc_means[i]<-auc(svm.predy, df$a)
detection_rate[i] <- h$d_rate
false_alarm_rate[i] <- h$f_rate
}
#Output
list(call = model, K = K,
svm_error_rate = mean(CV),
seed = seed, auc_mean = mean(auc_means),
d_rate = mean(detection_rate), f_rate = mean(false_alarm_rate))
}
er2<-cv.rf(data = haiti_data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=17)
# this is my custom function to calculate False Alarm Rate and Detection Rate
# detection Rate = Number of correct Blue Tarps/ number of total blue tarps
# false alarm rate = number of incorrect blue tarps/(number of non-blue tarps)
custom_err <- function(df){
actual <- df['a']
pred <- df['p']
#newvec <-  ifelse(actual==0 & pred==0, "TN",ifelse(actual==0 & pred==1, "FP",
#                                                   ifelse(actual==1 & pred==0, "FN", "TP")))
#0 for Blue Tarp
#1 for Not Blue Tarp
correct_tarps <- ifelse(actual==0 & pred == 0, TRUE, FALSE)
blue_tarps <- ifelse(actual==0, TRUE, FALSE)
correct_tarps_cnt <- sum(correct_tarps, na.rm = TRUE)
blue_tarps_cnt <- sum(blue_tarps, na.rm = TRUE)
incorrect_blue_tarps <- ifelse(actual==1 & pred == 0, TRUE, FALSE)
non_blue_tarps <- blue_tarps <- ifelse(actual==1, TRUE, FALSE)
detection_rate <- correct_tarps_cnt/blue_tarps_cnt
#f_alarm_rate <- (length(newvec[newvec=='FP']))/(length(newvec[newvec=='FP'])+length(newvec[newvec=='TN']))
f_alarm_rate <- (sum(incorrect_blue_tarps, na.rm = TRUE)/sum(non_blue_tarps, na.rm = TRUE))
list(d_rate=as.numeric(detection_rate), f_rate = as.numeric(f_alarm_rate))
}
er2<-cv.rf(data = haiti_data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=17)
er2$svm_error_rate
er2$auc_mean
er2$d_rate
er2$f_rate
cv.rf <-
function (data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=123) {
n <- nrow(data)
set.seed(seed)
datay=data[,yname] #response variable
#partition the data into K subsets
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n)
#generate indices 1:10 and sample n of them
# K fold cross-validated error
CV=NULL
auc_means <- rep(0,10)
detection_rate <- rep(0,10)
false_alarm_rate <- rep(0,10)
for (i in 1:K) { #i=1
test.index <- seq_len(n)[(s == i)] #test data
train.index <- seq_len(n)[(s != i)] #training data
#model with training data
rf.fit <-randomForest(model,data=data[train.index,],mtry=1, ntree = 250)
#predict
nrow(data[test.index,])
rf.predy = predict(rf.fit,data[test.index,])
error= mean(data[test.index, yname]!=as.factor(rf.predy))
#error rates
CV=c(CV,error)
#auc curve
#HAVEN"T FIGURED THIS PART OUT
#pred.k = prediction(predict(lda.fit, data[test.index,])$posterior[,2], data[test.index,]$is_blue_tarp)
#auc.k = performance(pred.k, "auc")
#auc_means[i] <- auc.k@y.values[[1]]
a <-ifelse(data[test.index,]$is_blue_tarp == 'False', 1,0)
p <- ifelse(as.factor(rf.predy) == "False", 1,0)
df = data.frame(a,p)
h <- custom_err(df)
auc_means[i]<-auc(rf.predy, df$a)
detection_rate[i] <- h$d_rate
false_alarm_rate[i] <- h$f_rate
}
#Output
list(call = model, K = K,
svm_error_rate = mean(CV),
seed = seed, auc_mean = mean(auc_means),
d_rate = mean(detection_rate), f_rate = mean(false_alarm_rate))
}
er2<-cv.rf(data = haiti_data, model=is_blue_tarp ~ Red + Green + Blue, yname="is_blue_tarp", K=10, seed=17)
er2$rf_error_rate
er2$auc_mean
er2$d_rate
er2$f_rate
setwd("~/uva_dsi/SYS6018/project1")
train_data = read.csv("HaitiPixels.csv")
setwd("~/uva_dsi/SYS6018/project1")
setwd("~/uva_dsi/SYS6018/project_final/Hold Out Data")
test_data = read.csv("HoldOut_Pixels.csv")
#libraries
library(caret)
length(unique(train_data$Class))
train_data$is_blue_tarp <- factor(ifelse(train_data$Class == "Blue Tarp", "True", "False"))
set.seed(17)
summary(test_data)
#################
#LOGISTIC REGRESSION
#################
glm.fit.train <- glm(is_blue_tarp ~ Red + Green + Blue,
data = train_data,
family = binomial)
glm.pred <- rep("False", nrow(test_data))
glm.probs <- predict(glm.fit.train, test_data, type = 'response')
glm.pred[glm.probs>0.5]<- "True"
confusionMatrix(as.factor(glm.pred), test_data$is_blue_tarp)
#detection rate
d_rate <- 14310/(14310+170)
d_rate <- 14310/(14310+170)
d_rate
#false alarm rate
f_rate <- 20314/(20314+1969383)
f_rate
#AUC
library(ROCR)
pred.log.glm <- prediction(glm.probs, test_data$is_blue_tarp)
perf.log.glm <- performance(pred.log.glm, "tpr", "fpr")
auc.log.glm <- performance(pred.log.glm, 'auc')
auc.log.glm@y.values[[1]]
#model with training data
lda.fit=lda(is_blue_tarp ~ Red + Green + Blue, data=train_data)
#predicted test set
lda.pred=predict(lda.fit, test_data)$class
confusionMatrix(lda.pred, test_data$is_blue_tarp)
d_rate <- 12148/(12148+2332)
d_rate
#false alarm rate
f_rate <- 34245/(34245+1955452)
f_rate
pred.lda = prediction(predict(lda.fit, test_data)$posterior[,2], test_data$is_blue_tarp)
auc.lda = performance(pred.lda, "auc")
auc.lda@y.values[[1]]
#model with training data
qda.fit=qda(is_blue_tarp ~ Red + Green + Blue, data=train_data)
#predicted test set
qda.pred=predict(qda.fit, test_data)$class
confusionMatrix(qda.pred, test_data$is_blue_tarp)
#detection rate
d_rate <- 10058/(10058+4422)
d_rate
#false alarm rate
f_rate <- 3651/(3651+1986046)
f_rate
#auc curve
library(ROCR)
pred.qda = prediction(predict(qda.fit, test_data)$posterior[,2], test_data$is_blue_tarp)
auc.qda = performance(pred.qda, "auc")
auc.qda@y.values[[1]]
#################
#KNN
#################
library(class)
library(caret)
train.X <- data.frame(train_data$Red, train_data$Green, train_data$Blue)
test.X <- data.frame(test_data$Red, test_data$Green, test_data$Blue)
#changed to test_sample
train_data.is_blue_tarp<- train_data$is_blue_tarp
knn.pred = knn(train.X, test.X, train_data.is_blue_tarp, k=6)
confusionMatrix(knn.pred, test_sample$is_blue_tarp)
confusionMatrix(knn.pred, test_data$is_blue_tarp)
#detection rate
d_rate <- 11890/(11890+2590)
d_rate
#false alarm rate
f_rate <- 12843/(1976854+12843)
f_rate
rf.fit <-randomForest(is_blue_tarp ~ Red + Green + Blue, data=train_data, mtry=1, ntree = 250)
rf.predy = predict(rf.fit, test_data, type = 'response')
as.factor(rf.predy)
confusionMatrix(test_data$is_blue_tarp, as.factor(rf.predy))
library(pROC)
library(ROCR)
#detection rate
d_rate <- 11178/(11178+6915)
d_rate
#false alarm rate
f_rate <- 3302/(3302+1982782)
f_rate
auc(rf.predy, test_data$is_blue_tarp)
1982782+3302
12843+1976854
1982782+3302+6915+11178
12843+1976854+2590+11890
#detection rate
d_rate <- 11178/(11178+6915)
d_rate
#false alarm rate
f_rate <- 3302/(3302+1982782)
f_rate
#################
#SVM (Kernel = Radial)
#################
library(e1071)
svm.fit <- svm(is_blue_tarp ~ Red + Green + Blue, kernel = 'radial', data = train_data, cost = 5, gamma = 50)
svm.pred = predict(svm.fit, test_data)
#might have to put p in place of svm.pred
confusionMatrix(svm.pred, test_data$is_blue_tarp)
#detection rate
d_rate <- 10355/(10355+4125)
d_rate
#false alarm rate
f_rate <- 9601/(9601+1980096)
f_rate
11890+2590
10058+4422
12148+2332
14310+170
rf.predy = predict(rf.fit, test_data)
confusionMatrix(test_data$is_blue_tarp, rf.predy)
3295+11185
summary(test_data$is_blue_tarp)
##confusion matrix must be flipped here..
#detection rate
d_rate <- 11185/(11185+1989697)
d_rate
##confusion matrix must be flipped here..
#detection rate
d_rate <- 11185/(11185+3295)
d_rate
#false alarm rate
f_rate <- 6907/(1989697)
f_rate
glm.fit.train <- glm(is_blue_tarp ~ Red + Green + Blue,
data = train_data,
family = binomial)
glm.pred <- rep("False", nrow(test_data))
glm.probs <- predict(glm.fit.train, test_data, type = 'response')
glm.pred[glm.probs>0.5]<- "True"
confusionMatrix(as.factor(glm.pred), test_data$is_blue_tarp)
#might have to put p in place of svm.pred
confusionMatrix(test_data$is_blue_tarp, svm.pred)
confusionMatrix(rf.predy, test_data$is_blue_tarp)
